{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "superb-tactics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from memory_profiler import memory_usage\n",
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import rpy2.rinterface\n",
    "import rpy2_arrow.pyarrow_rarrow as pyra\n",
    "import pyarrow.feather as feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "noted-compatibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taogu\\miniconda3\\envs\\525\\lib\\site-packages\\rpy2\\robjects\\packages.py:366: UserWarning: The symbol 'quartz' is not in this R namespace/package.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%load_ext rpy2.ipython\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-dispute",
   "metadata": {},
   "source": [
    "# 1. Teamwork contract\n",
    "\n",
    "The teamwork contract for our team, **team 17**, can be found [**here**](https://docs.google.com/document/d/15_jlrMTtFVXrCJXXRBJv0j-UZxT8hCKHzhsw8ymYr8o/edit?usp=sharing).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-terry",
   "metadata": {},
   "source": [
    " # 2. Create repository and project structure\n",
    " \n",
    " The repository URL: **https://github.com/UBC-MDS/525_group17**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-accessory",
   "metadata": {},
   "source": [
    "# 3. Download the data\n",
    "We will get the data using the figshare API.\n",
    "\n",
    "First we need to ensure we are in the root directory and create the necessary directories for the raw and combined data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accredited-newman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\MDS_Block_6\\\\525_group17\\\\notebooks'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "stupid-stand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\MDS_Block_6\\525_group17\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "rubber-question",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined_data\n",
      "data\n",
      "img\n",
      "LICENSE\n",
      "notebooks\n",
      "README.md\n"
     ]
    }
   ],
   "source": [
    "!rm -r data\n",
    "!mkdir data\n",
    "!rm -r combined_data\n",
    "!mkdir combined_data\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "amber-sharing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary metadata for using figshare API\n",
    "article_id = 14096681\n",
    "url = f\"https://api.figshare.com/v2/articles/{article_id}\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "output_directory = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "german-choir",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "data = json.loads(response.text)  # this contains all the articles data, feel free to check it out\n",
    "files = data[\"files\"]             # this is just the data about the files, which is what we want\n",
    "# files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-dollar",
   "metadata": {},
   "source": [
    "The file we want is `data.zip`. We will download it with `urllib.request.urlretrieve()` then extract it with `zipfile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "charged-embassy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "files_to_dl = [\"data.zip\"] \n",
    "for file in files:\n",
    "    if file[\"name\"] in files_to_dl:\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        urlretrieve(file[\"download_url\"], output_directory + file[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "silent-colon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 53.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with zipfile.ZipFile(os.path.join(output_directory, \"data.zip\"), 'r') as f:\n",
    "    f.extractall(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-contents",
   "metadata": {},
   "source": [
    "### Result Comparison\n",
    "| | Team member| Operating System | CPU | RAM | Run-time|\n",
    "|:---:|:----------:|:----------------:|:---:|:---:|:-------:|\n",
    "|**Machine 1**|Lara Habashy|   MacOS      |   Intel Core i5  |  16GB   |  1min 37s      |\n",
    "|**Machine 2**|Cameron Harris|    MacOS            |  Intel Core i7   |  16GB   |   1min 45s      |\n",
    "|**Machine 3**|Trevor Kinsey|   Windows 10 Pro     |  Intel Core i7-1065G7   | 16GB  |  15min 42s   |\n",
    "|**Machine 4**|Guanshu Tao|      Windows 10 Pro          |  10th Generation Intel Core i5-10210U   |   16GB     |    1min 11s       |\n",
    "\n",
    "### Discussion \n",
    "- The data was downloaded fairly quickly, except for Trevor, whose internet connection is slow. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-alcohol",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 4. Combining data CSVs\n",
    "There are now many `.csv` files that we want to merge into a single file. \n",
    "\n",
    "All the files but one have the same columns:\n",
    "- `time`\n",
    "- `lat_min`\n",
    "- `lat_max`\n",
    "- `lon_min`\n",
    "- `lon_max`\n",
    "- `rain (mm/day)`) \n",
    "\n",
    "The file `observed_daily_rainfall_SYD.csv` only has:\n",
    "- `time`\n",
    "- `rain (mm/day)`\n",
    "\n",
    "I wish to keep all these columns and add data to the missing columns from `observed_daily_rainfall_SYD.csv` by looking up the latitude and longitude information for this data. It seems to be from Sydney. For now this missing data will appear as NaN's in the combined dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-effort",
   "metadata": {},
   "source": [
    "### Using pandas to combine data\n",
    "One of the files, `observed_daily_rainfall_SYD.csv` was missing 4 columns the others had, so we made them and filled them with NaNs. Also we made a column 'model' that says which file the data came from. \n",
    "\n",
    "It took us 7-10 minutes to combine and save the data as `combined_data.csv`, then about 1 minute to load the file back into a dataframe. This is a big dataframe, with over 62 million rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "female-format",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time', 'rain (mm/day)']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cols = pd.read_csv(\"data/ACCESS-CM2_daily_rainfall_NSW.csv\").columns.to_list()\n",
    "obs_df = pd.read_csv(\"data/observed_daily_rainfall_SYD.csv\")\n",
    "obs_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "iraqi-latex",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df = pd.read_csv(\"data/observed_daily_rainfall_SYD.csv\")\n",
    "obs_df.insert(1, \"lat_min\", np.nan, True)\n",
    "obs_df.insert(2, \"lat_max\", np.nan, True)\n",
    "obs_df.insert(3, \"lon_min\", np.nan, True)\n",
    "obs_df.insert(4, \"lon_max\", np.nan, True)\n",
    "obs_df = obs_df.reindex(columns = use_cols)\n",
    "\n",
    "obs_df.to_csv(\"data/observed_daily_rainfall_SYD.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bridal-nickel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 306.81 MiB, increment: 0.07 MiB\n",
      "Wall time: 26min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit\n",
    "# combine data into a single giant `combined_data.csv` file\n",
    "files = glob.glob('data/*.csv')\n",
    "df = pd.concat((pd.read_csv(file)\n",
    "                .assign(model=re.findall(r'(?<=data\\\\)[^\\/]+(?=\\_d)', file)[0])\n",
    "                for file in files)\n",
    "              )\n",
    "df.to_csv(\"combined_data/combined_data.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "empty-boring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# df = pd.read_csv(\"combined_data/combined_data.csv\", index_col=0)\n",
    "df = pd.read_csv(\"combined_data/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "stable-berry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.7 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>lat_min</th>\n",
       "      <th>lat_max</th>\n",
       "      <th>lon_min</th>\n",
       "      <th>lon_max</th>\n",
       "      <th>rain (mm/day)</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1889-01-01 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>3.293256e-13</td>\n",
       "      <td>ACCESS-CM2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1889-01-02 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>ACCESS-CM2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1889-01-03 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>ACCESS-CM2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1889-01-04 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>ACCESS-CM2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1889-01-05 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>1.047658e-02</td>\n",
       "      <td>ACCESS-CM2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time  lat_min  lat_max  lon_min  lon_max  rain (mm/day)  \\\n",
       "0  1889-01-01 12:00:00   -36.25    -35.0  140.625    142.5   3.293256e-13   \n",
       "1  1889-01-02 12:00:00   -36.25    -35.0  140.625    142.5   0.000000e+00   \n",
       "2  1889-01-03 12:00:00   -36.25    -35.0  140.625    142.5   0.000000e+00   \n",
       "3  1889-01-04 12:00:00   -36.25    -35.0  140.625    142.5   0.000000e+00   \n",
       "4  1889-01-05 12:00:00   -36.25    -35.0  140.625    142.5   1.047658e-02   \n",
       "\n",
       "        model  \n",
       "0  ACCESS-CM2  \n",
       "1  ACCESS-CM2  \n",
       "2  ACCESS-CM2  \n",
       "3  ACCESS-CM2  \n",
       "4  ACCESS-CM2  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "widespread-exclusive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62513863, 7)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-execution",
   "metadata": {},
   "source": [
    "## <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-colon",
   "metadata": {},
   "source": [
    "### Using DASK to combine data\n",
    "\n",
    "DASK took slightly longer  to combine the data into one .csv file compared to  pandas. \n",
    "\n",
    "DASK is able to load the dataframe much faster using `dd.read_csv()`.\n",
    "\n",
    "The `combined_data_dask.csv` file made from a DASK ddf takes up more space on disk as the `combined_data.csv` file made from a pandas df. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "earned-bailey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 10700.45 MiB, increment: 5541.69 MiB\n",
      "Wall time: 9min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "ddf = dd.read_csv(\"data/*.csv\", \n",
    "                  include_path_column = True,\n",
    "                  assume_missing = True)\n",
    "ddf.to_csv(\"combined_data/combined_data_dask.csv\", \n",
    "           single_file = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "certain-malawi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 35.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ddf = dd.read_csv(\"combined_data/combined_data_dask.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "labeled-strip",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 709 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>time</th>\n",
       "      <th>lat_min</th>\n",
       "      <th>lat_max</th>\n",
       "      <th>lon_min</th>\n",
       "      <th>lon_max</th>\n",
       "      <th>rain (mm/day)</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1889-01-01 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>3.293256e-13</td>\n",
       "      <td>C:/Users/Trevor_Kinsey/MDS/Block_6/DSCI_525/52...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1889-01-02 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>C:/Users/Trevor_Kinsey/MDS/Block_6/DSCI_525/52...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1889-01-03 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>C:/Users/Trevor_Kinsey/MDS/Block_6/DSCI_525/52...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1889-01-04 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>C:/Users/Trevor_Kinsey/MDS/Block_6/DSCI_525/52...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1889-01-05 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>1.047658e-02</td>\n",
       "      <td>C:/Users/Trevor_Kinsey/MDS/Block_6/DSCI_525/52...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                 time  lat_min  lat_max  lon_min  lon_max  \\\n",
       "0           0  1889-01-01 12:00:00   -36.25    -35.0  140.625    142.5   \n",
       "1           1  1889-01-02 12:00:00   -36.25    -35.0  140.625    142.5   \n",
       "2           2  1889-01-03 12:00:00   -36.25    -35.0  140.625    142.5   \n",
       "3           3  1889-01-04 12:00:00   -36.25    -35.0  140.625    142.5   \n",
       "4           4  1889-01-05 12:00:00   -36.25    -35.0  140.625    142.5   \n",
       "\n",
       "   rain (mm/day)                                               path  \n",
       "0   3.293256e-13  C:/Users/Trevor_Kinsey/MDS/Block_6/DSCI_525/52...  \n",
       "1   0.000000e+00  C:/Users/Trevor_Kinsey/MDS/Block_6/DSCI_525/52...  \n",
       "2   0.000000e+00  C:/Users/Trevor_Kinsey/MDS/Block_6/DSCI_525/52...  \n",
       "3   0.000000e+00  C:/Users/Trevor_Kinsey/MDS/Block_6/DSCI_525/52...  \n",
       "4   1.047658e-02  C:/Users/Trevor_Kinsey/MDS/Block_6/DSCI_525/52...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "overhead-married",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11G\tcombined_data/combined_data_dask.csv\n",
      "5.7G\tcombined_data/combined_data.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "du -sh combined_data/combined_data_dask.csv\n",
    "du -sh combined_data/combined_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-writer",
   "metadata": {},
   "source": [
    "### Result Comparison \n",
    "| | Team member| Operating System | CPU | RAM | Run-time (using pandas)|  Run-time (using DASK)| \n",
    "|:---:|:----------:|:----------------:|:---:|:---:|:-------:|:--------:|\n",
    "|**Machine 1**|Lara Habashy|   MacOS      |   Intel Core i7  |  16GB   |  10min 10s       | 12min 31s |\n",
    "|**Machine 2**|Cameron Harris|    MacOS            |  Intel Core i7   |  16GB   |    8min 50s     | - |\n",
    "|**Machine 3**|Trevor Kinsey|   Windows 10 Pro     |  Intel Core i7-1065G7   | 16GB  | 6min 56s |10min 20s |\n",
    "|**Machine 4**|Guanshu Tao|      Windows 10 Pro          |  10th Generation Intel Core i5-10210U   |   16GB     | 8min 50s         |58min 37s|\n",
    "\n",
    "#### Discussion \n",
    "- This was a slow process. \n",
    "- A few of us had difficulty combining the data with DASK. Those who did Use DASK didn't see any advantage over using pandas when it comes to combining the data.\n",
    "\n",
    "- **Conclusion:** Pandas seems to be the better of these two options because it was faster and the file it created was smaller.\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-dallas",
   "metadata": {},
   "source": [
    "# 5. Load the combined CSV to memory and perform a simple EDA\n",
    "\n",
    "The EDA will be to use `value_counts()` to count the number of data points that came from each .csv file, as recorded in the `model` column of `combined_data.csv`.\n",
    "\n",
    "We will try and assess several methods do do this then choose our favourite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-queue",
   "metadata": {},
   "source": [
    "### 5.1 Pandas (no chunking)\n",
    "\n",
    "We will take this method to be a baseline with which to compare other methods. Note that the entire dataframe requires 8GB of memory.\n",
    "\n",
    "**Verdict:** It's annoyingly slow, but not impossible to open a large file. Memory use could be a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "neutral-superior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 9974.68 MiB, increment: 8330.80 MiB\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "df = pd.read_csv(\"combined_data/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "plain-growth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI-ESM1-2-HR       5154240\n",
      "NorESM2-MM          3541230\n",
      "CMCC-CM2-HR4        3541230\n",
      "TaiESM1             3541230\n",
      "CMCC-CM2-SR5        3541230\n",
      "CMCC-ESM2           3541230\n",
      "SAM0-UNICON         3541153\n",
      "GFDL-ESM4           3219300\n",
      "GFDL-CM4            3219300\n",
      "FGOALS-f3-L         3219300\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "MRI-ESM2-0          3037320\n",
      "BCC-CSM2-MR         3035340\n",
      "MIROC6              2070900\n",
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "INM-CM5-0           1609650\n",
      "INM-CM4-8           1609650\n",
      "KIOST-ESM           1287720\n",
      "FGOALS-g3           1287720\n",
      "AWI-ESM-1-1-LR       966420\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "NESM3                966420\n",
      "MPI-ESM1-2-LR        966420\n",
      "NorESM2-LM           919800\n",
      "BCC-ESM1             551880\n",
      "CanESM5              551880\n",
      "observed              46020\n",
      "Name: model, dtype: int32\n",
      "peak memory: 6664.56 MiB, increment: 60.50 MiB\n",
      "Wall time: 4.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "counts = df[\"model\"].value_counts()\n",
    "print(counts.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-oliver",
   "metadata": {},
   "source": [
    "### 5.2 Pandas (with chunking)\n",
    "\n",
    "This took slightly less time as pandas without chunking, but used much less memory. \n",
    "\n",
    "The effect of chunk size:\n",
    "- smaller chunk size (1,000,000) takes about the same time as larger chunk size (10,000,000)\n",
    "- smaller chunk size (1,000,000) uses less memory than larger chunk size (10,000,000)\n",
    "\n",
    "We could not time the loading of the data and performing the EDA (`value_counts()`) separately because the data from each chunk had to be counted as it became available. However the time  to do the combined operations using chunking was almost the same as doing the same operations in sequence without chunking.\n",
    "\n",
    "**Verdict:** Chunking decreases the memory needed but doesn't save much time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "artistic-matter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 6822.82 MiB, increment: 218.68 MiB\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "CHUNKSIZE = 1_000_000\n",
    "counts = pd.Series(dtype=int)\n",
    "for chunk in pd.read_csv(\"combined_data/combined_data.csv\",\n",
    "                         chunksize=CHUNKSIZE):\n",
    "    counts = counts.add(chunk[\"model\"].value_counts(), fill_value=0)\n",
    "\n",
    "# suppress output to avoid repeating lengthy printout    \n",
    "# print(counts.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "outside-cable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "AWI-ESM-1-1-LR       966420\n",
      "BCC-CSM2-MR         3035340\n",
      "BCC-ESM1             551880\n",
      "CMCC-CM2-HR4        3541230\n",
      "CMCC-CM2-SR5        3541230\n",
      "CMCC-ESM2           3541230\n",
      "CanESM5              551880\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "FGOALS-f3-L         3219300\n",
      "FGOALS-g3           1287720\n",
      "GFDL-CM4            3219300\n",
      "GFDL-ESM4           3219300\n",
      "INM-CM4-8           1609650\n",
      "INM-CM5-0           1609650\n",
      "KIOST-ESM           1287720\n",
      "MIROC6              2070900\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "MPI-ESM1-2-HR       5154240\n",
      "MPI-ESM1-2-LR        966420\n",
      "MRI-ESM2-0          3037320\n",
      "NESM3                966420\n",
      "NorESM2-LM           919800\n",
      "NorESM2-MM          3541230\n",
      "SAM0-UNICON         3541153\n",
      "TaiESM1             3541230\n",
      "observed              46020\n",
      "dtype: int32\n",
      "peak memory: 8377.54 MiB, increment: 1720.98 MiB\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "CHUNKSIZE = 10_000_000\n",
    "counts = pd.Series(dtype=int)\n",
    "for chunk in pd.read_csv(\"combined_data/combined_data.csv\", chunksize=CHUNKSIZE):\n",
    "    counts = counts.add(chunk[\"model\"].value_counts(), fill_value=0)\n",
    "print(counts.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-stanley",
   "metadata": {},
   "source": [
    "### 5.3 Pandas (loading only some columns, no chunking)\n",
    "Since we only want the model for EDA, we will import just the `model` column. This is faster and uses less memory than loading the whole dataframe. \n",
    "\n",
    "Running `value_counts` takes the same time as it did using the entire data set, probably because it has to iterate through the same number of rows.\n",
    "\n",
    "**Verdict:** This should be done whenever possible. It reduces memory required and speeds up loading data but has no effect on EDA time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "secret-carnival",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 6889.91 MiB, increment: 958.11 MiB\n",
      "Wall time: 36.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "df = pd.read_csv(\"combined_data/combined_data.csv\", \n",
    "                 usecols = [\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "unlikely-adjustment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 1508.04 MiB, increment: 110.69 MiB\n",
      "Wall time: 4.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "df[\"model\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-triple",
   "metadata": {},
   "source": [
    "### 5.4 DASK dataframe\n",
    "\n",
    "This method is much faster than using pandas (without chunking), and uses less memory. The memory use is similar to using pandas with chunking, depending on the chunk size used. The `value_counts()` step took longer than the baseline pandas method because of the `.compute()` step, which is required when we want to work with the data and view the result.\n",
    "\n",
    "**Verdict:** Fast and light on memory. Similar to chunked pandas but easier to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "surprising-european",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 1402.02 MiB, increment: 4.66 MiB\n",
      "Wall time: 2.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# load data\n",
    "ddf = dd.read_csv(\"combined_data/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "competitive-feature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI-ESM1-2-HR       5154240\n",
      "TaiESM1             3541230\n",
      "CMCC-CM2-HR4        3541230\n",
      "CMCC-CM2-SR5        3541230\n",
      "CMCC-ESM2           3541230\n",
      "NorESM2-MM          3541230\n",
      "SAM0-UNICON         3541153\n",
      "FGOALS-f3-L         3219300\n",
      "GFDL-CM4            3219300\n",
      "GFDL-ESM4           3219300\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "MRI-ESM2-0          3037320\n",
      "BCC-CSM2-MR         3035340\n",
      "MIROC6              2070900\n",
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "INM-CM4-8           1609650\n",
      "INM-CM5-0           1609650\n",
      "KIOST-ESM           1287720\n",
      "FGOALS-g3           1287720\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "MPI-ESM1-2-LR        966420\n",
      "NESM3                966420\n",
      "AWI-ESM-1-1-LR       966420\n",
      "NorESM2-LM           919800\n",
      "CanESM5              551880\n",
      "BCC-ESM1             551880\n",
      "observed              46020\n",
      "Name: model, dtype: int64\n",
      "peak memory: 2973.74 MiB, increment: 1576.16 MiB\n",
      "Wall time: 38.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# Do EDA\n",
    "print(ddf[\"model\"].value_counts().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-brief",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-business",
   "metadata": {},
   "source": [
    "### 5.5 Changing dtype of the data\n",
    "\n",
    "Four of the columns are in float64 format so they could be converted to float32 and reduce the memory required by 1.25GB. \n",
    "\n",
    "**Verdict:** Should be done regardless of the method used to store and import data. This is low-hanging fruit to save space and should be used whenever possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "herbal-strap",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage with float64: 2500.55 MB\n",
      "Memory usage with float32: 1250.28 MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"combined_data/combined_data.csv\")\n",
    "\n",
    "print(f\"Memory usage with float64: {df[['lat_min', 'lat_max', 'lon_min', 'lon_max', 'rain (mm/day)']].memory_usage().sum() / 1e6:.2f} MB\")\n",
    "print(f\"Memory usage with float32: {df[['lat_min', 'lat_max', 'lon_min', 'lon_max', 'rain (mm/day)']].astype('float32', errors='ignore').memory_usage().sum() / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-provincial",
   "metadata": {},
   "source": [
    "### Result Comparison\n",
    "| | Team member| Operating System | CPU | RAM | Run-time (using pandas)|  Run-time (using DASK)| Chunking | \n",
    "|:---:|:----------:|:----------------:|:---:|:---:|:-------:|:--------:|:----:|\n",
    "|**Machine 1**|Lara Habashy|   MacOS      |   Intel Core i5  |  16GB   |   1min 10s     | 48s  |2min 18s |\n",
    "|**Machine 2**|Cameron Harris|   MacOS             |  Intel Core i7   |  16GB   |    1min 50s     | - | 2min 10s|\n",
    "|**Machine 3**|Trevor Kinsey|   Windows 10 Pro     |  Intel Core i7-1065G7   | 16GB  | 1min 8s  |  41s   |  1min 6s  |\n",
    "|**Machine 4**|Guanshu Tao|    Windows 10 Pro          |  10th Generation Intel Core i5-10210U   |   16GB     |  1min 23s       |45s|1min 27s|\n",
    "\n",
    "\n",
    "#### Discussion \n",
    "- Changing dtype and selecting only the columns you want are both effective in reducing time to load data and the memory required\n",
    "- Using DASK to open and do EDA was faster than using pandas, but has proved to be unreliable because not all of us could get it to work.\n",
    "- **Conclusion:** Change the dtype of the data and selecting only the columns you want whenever possible. Use DASK if you can."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-seminar",
   "metadata": {},
   "source": [
    "# 6. Perform a simple EDA in R\n",
    "To do this we will pass data from python to R in various ways, asses each method, then decide which one is best suited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "amber-logistics",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: \n",
      "Attaching package: 'dplyr'\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from 'package:stats':\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from 'package:base':\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "library(dplyr)\n",
    "library(arrow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-showcase",
   "metadata": {},
   "source": [
    "### 6.1 Using %%R -i\n",
    "\n",
    "This didn't work because it seems our laptops didn't have enough memory, probably because we needed to have two large data frames (in python and R) open at the same time. This doesn't seem to be a good way to pass a large dataframe from python to R.\n",
    "\n",
    "**Verdict:** NO THANKS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "intermediate-village",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# %%R -i df\n",
    "\n",
    "# start_time <- Sys.time()\n",
    "# library(dplyr)\n",
    "# counts <- df %>% count(model)\n",
    "# end_time <- Sys.time()\n",
    "\n",
    "# print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-marketing",
   "metadata": {},
   "source": [
    "### 6.2 Using arrow table and pyra\n",
    "\n",
    "The arrow table is an intermediary form that can easily be moved between python and R. Once it is made it can be convert it using `pyra.converter.py2rpy()` to a table that is readble in R. This is a rather roundabout way to get data into R. It would be nicer to be able to load a file directly into R.\n",
    "\n",
    "To create a table from file in python, convert the table, then pass to R took about the same time as loading the file and doing the EDA in python. This is impressive, since it took the same amount of time to do much more. \n",
    "\n",
    "**Verdict: Ok, but inconvenient.** This method works and but it is inconvenient to have to create a table in python, convert it, then pass that to R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "charitable-holocaust",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 8649.15 MiB, increment: 3493.14 MiB\n",
      "Wall time: 1min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# Read data file and prepare arrow table\n",
    "dataset = ds.dataset(\"combined_data/combined_data.csv\", format=\"csv\")\n",
    "table = dataset.to_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "divine-zambia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5756\n",
      "rarrow.ChunkedArray: 0.11634445190429688\n",
      "5756\n",
      "rarrow.ChunkedArray: 0.06902265548706055\n",
      "5756\n",
      "rarrow.ChunkedArray: 0.07774066925048828\n",
      "5756\n",
      "rarrow.ChunkedArray: 0.07131099700927734\n",
      "5756\n",
      "rarrow.ChunkedArray: 0.09745645523071289\n",
      "5756\n",
      "rarrow.ChunkedArray: 0.1260051727294922\n",
      "5756\n",
      "rarrow.ChunkedArray: 0.12061429023742676\n",
      "peak memory: 8644.70 MiB, increment: 72.16 MiB\n",
      "Wall time: 1min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# convert arrow table so it can be passed to R\n",
    "r_table = pyra.converter.py2rpy(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "identified-negotiation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# A tibble: 28 x 2\n",
      "   model                  n\n",
      "   <chr>              <int>\n",
      " 1 ACCESS-CM2       1932840\n",
      " 2 ACCESS-ESM1-5    1610700\n",
      " 3 AWI-ESM-1-1-LR    966420\n",
      " 4 BCC-CSM2-MR      3035340\n",
      " 5 BCC-ESM1          551880\n",
      " 6 CanESM5           551880\n",
      " 7 CMCC-CM2-HR4     3541230\n",
      " 8 CMCC-CM2-SR5     3541230\n",
      " 9 CMCC-ESM2        3541230\n",
      "10 EC-Earth3-Veg-LR 3037320\n",
      "# ... with 18 more rows\n",
      "Time difference of 5.679571 secs\n",
      "Wall time: 5.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R -i r_table\n",
    "# Pass r_table from python\n",
    "\n",
    "start_time <- Sys.time()\n",
    "library(dplyr)\n",
    "counts <- r_table %>% collect() %>% count(model)\n",
    "end_time <- Sys.time()\n",
    "\n",
    "print(counts)\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-cable",
   "metadata": {},
   "source": [
    "### 6.3 Using feather\n",
    "If we have an arrow table it can be saved as a .feather file, which R can read directly. \n",
    "\n",
    "**Verdict:** This method was very fast for those who could get it to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "declared-split",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create .feather file from existing arrow table\n",
    "feather.write_feather(table, 'combined_data/combined_data.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "studied-merit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time difference of 6.747203 secs\n",
      "Wall time: 6.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "# Read .feather file into R\n",
    "library(arrow)\n",
    "start_time <- Sys.time()\n",
    "r_table <- arrow::read_feather(\"combined_data/combined_data.feather\")\n",
    "end_time <- Sys.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dangerous-advantage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;246m# A tibble: 28 x 2\u001b[39m\n",
      "   model                  n\n",
      " \u001b[38;5;250m*\u001b[39m \u001b[3m\u001b[38;5;246m<chr>\u001b[39m\u001b[23m              \u001b[3m\u001b[38;5;246m<int>\u001b[39m\u001b[23m\n",
      "\u001b[38;5;250m 1\u001b[39m ACCESS-CM2       1\u001b[4m9\u001b[24m\u001b[4m3\u001b[24m\u001b[4m2\u001b[24m840\n",
      "\u001b[38;5;250m 2\u001b[39m ACCESS-ESM1-5    1\u001b[4m6\u001b[24m\u001b[4m1\u001b[24m\u001b[4m0\u001b[24m700\n",
      "\u001b[38;5;250m 3\u001b[39m AWI-ESM-1-1-LR    \u001b[4m9\u001b[24m\u001b[4m6\u001b[24m\u001b[4m6\u001b[24m420\n",
      "\u001b[38;5;250m 4\u001b[39m BCC-CSM2-MR      3\u001b[4m0\u001b[24m\u001b[4m3\u001b[24m\u001b[4m5\u001b[24m340\n",
      "\u001b[38;5;250m 5\u001b[39m BCC-ESM1          \u001b[4m5\u001b[24m\u001b[4m5\u001b[24m\u001b[4m1\u001b[24m880\n",
      "\u001b[38;5;250m 6\u001b[39m CanESM5           \u001b[4m5\u001b[24m\u001b[4m5\u001b[24m\u001b[4m1\u001b[24m880\n",
      "\u001b[38;5;250m 7\u001b[39m CMCC-CM2-HR4     3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[38;5;250m 8\u001b[39m CMCC-CM2-SR5     3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[38;5;250m 9\u001b[39m CMCC-ESM2        3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[38;5;250m10\u001b[39m EC-Earth3-Veg-LR 3\u001b[4m0\u001b[24m\u001b[4m3\u001b[24m\u001b[4m7\u001b[24m320\n",
      "\u001b[38;5;246m# ... with 18 more rows\u001b[39m\n",
      "Time difference of 2.271981 secs\n",
      "Wall time: 2.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "# Perform EDA in R\n",
    "library(dplyr)\n",
    "start_time <- Sys.time()\n",
    "counts <- r_table %>% collect() %>% count(model)\n",
    "end_time <- Sys.time()\n",
    "\n",
    "print(counts)\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-wesley",
   "metadata": {},
   "source": [
    "### 6.4 Using parquet\n",
    "\n",
    "Our tests show that using arrow and parquet to perform the following steps:\n",
    "- create an arrow table from `combined_data.csv` in python, \n",
    "- write the table to a parquet file, \n",
    "- read the parquet file into R, \n",
    "- do the EDA in R\n",
    "\n",
    "is very fast. Faster than simply reading the original `combined_data.csv`. The upside of this method is that we have a .parquet file that can ba accessed in the future without having to create the arrow table.\n",
    "\n",
    "**Verdict: This is the best choice in terms of speed and file size.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "valid-publicity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pq.write_table(table, 'combined_data/combined_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "worthy-neighborhood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time difference of 12.57204 secs\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "# Read .parquet file into R\n",
    "library(arrow)\n",
    "start_time <- Sys.time()\n",
    "r_table <- arrow::read_parquet(\"combined_data/combined_data.parquet\")\n",
    "end_time <- Sys.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "insured-specific",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# A tibble: 28 x 2\n",
      "   model                  n\n",
      "   <chr>              <int>\n",
      " 1 ACCESS-CM2       1932840\n",
      " 2 ACCESS-ESM1-5    1610700\n",
      " 3 AWI-ESM-1-1-LR    966420\n",
      " 4 BCC-CSM2-MR      3035340\n",
      " 5 BCC-ESM1          551880\n",
      " 6 CanESM5           551880\n",
      " 7 CMCC-CM2-HR4     3541230\n",
      " 8 CMCC-CM2-SR5     3541230\n",
      " 9 CMCC-ESM2        3541230\n",
      "10 EC-Earth3-Veg-LR 3037320\n",
      "# ... with 18 more rows\n",
      "Time difference of 2.032011 secs\n",
      "Wall time: 2.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "# Perform EDA in R\n",
    "library(dplyr)\n",
    "start_time <- Sys.time()\n",
    "counts <- r_table %>% collect() %>% count(model)\n",
    "end_time <- Sys.time()\n",
    "\n",
    "print(counts)\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-hypothesis",
   "metadata": {},
   "source": [
    "#### Compare filesize of various formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "contained-terrorist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.7G\tcombined_data/combined_data.csv\n",
      "1.1G\tcombined_data/combined_data.feather\n",
      "542M\tcombined_data/combined_data.parquet\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "du -sh combined_data/combined_data.csv\n",
    "du -sh combined_data/combined_data.feather\n",
    "du -sh combined_data/combined_data.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-scheduling",
   "metadata": {},
   "source": [
    "### Result Comparison\n",
    "| | Team member| Operating System | CPU | RAM | Run-time (arrow table + pyra)|  Run-time (feather)| Run-time (parquet) |\n",
    "|:---:|:----------:|:----------------:|:---:|:---:|:-------:|:--------:|:----:|\n",
    "|**Machine 1**|Lara Habashy|   MacOS      |  Intel Core i5   |  16GB   |    58s     | -  |  - |\n",
    "|**Machine 2**|Cameron Harris|    MacOS            |  Intel Core i7   |  16GB   |    56s     | - | - |\n",
    "|**Machine 3**|Trevor Kinsey|   Windows 10 Pro     |  Intel Core i7-1065G7   | 16GB  | 1min 5s |  -  | 1min 3 s    |\n",
    "|**Machine 4**|Guanshu Tao|    Windows 10 Pro          |  10th Generation Intel Core i5-10210U   |   16GB     |   1min 18s       | 9.3s| 26s|\n",
    "\n",
    "\n",
    "#### Discussion \n",
    "- The feather and parquet time include the time to create the arrow table. Once this is done, the time to open a file and perform EDA will be reduced.\n",
    "- Not everyone could get the .feather and .parquet file to work, but these methods are very fast when they do work. The .feather works faster than .parquet. \n",
    "- **Conclusion:** Use a .feather or .parquet file because they are both very fast, but they don't work for every computer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:525]",
   "language": "python",
   "name": "conda-env-525-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
