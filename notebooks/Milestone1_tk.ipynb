{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sublime-running",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from memory_profiler import memory_usage\n",
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import rpy2.rinterface\n",
    "import rpy2_arrow.pyarrow_rarrow as pyra\n",
    "import pyarrow.feather as feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "regional-theology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n",
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext rpy2.ipython\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-station",
   "metadata": {},
   "source": [
    "# 1. Teamwork contract\n",
    "\n",
    "See it at: \n",
    "\n",
    "https://docs.google.com/document/d/15_jlrMTtFVXrCJXXRBJv0j-UZxT8hCKHzhsw8ymYr8o/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-equilibrium",
   "metadata": {},
   "source": [
    " # 2. Create repository and project structure\n",
    " \n",
    " See it at: https://github.com/UBC-MDS/525_group17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-george",
   "metadata": {},
   "source": [
    "# 3. Download the data\n",
    "Get data using the figshare API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "portuguese-faculty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Trevor_Kinsey\\\\MDS\\\\Block_6\\\\DSCI_525'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "complicated-judge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Trevor_Kinsey\\MDS\\Block_6\\DSCI_525\\525_group17\n"
     ]
    }
   ],
   "source": [
    "# Not sure why the working directory is not the file location\n",
    "# This may be a bug specific to Trevor's setup\n",
    "\n",
    "cd 525_group17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "prescribed-sweden",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary metadata for using figshare API\n",
    "article_id = 14096681\n",
    "url = f\"https://api.figshare.com/v2/articles/{article_id}\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "output_directory = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beneficial-million",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'is_link_only': False,\n",
       "  'name': 'daily_rainfall_2014.png',\n",
       "  'supplied_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "  'computed_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "  'id': 26579150,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26579150',\n",
       "  'size': 58863},\n",
       " {'is_link_only': False,\n",
       "  'name': 'environment.yml',\n",
       "  'supplied_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "  'computed_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "  'id': 26579171,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26579171',\n",
       "  'size': 192},\n",
       " {'is_link_only': False,\n",
       "  'name': 'README.md',\n",
       "  'supplied_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "  'computed_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "  'id': 26586554,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26586554',\n",
       "  'size': 5422},\n",
       " {'is_link_only': False,\n",
       "  'name': 'data.zip',\n",
       "  'supplied_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "  'computed_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "  'id': 26766812,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26766812',\n",
       "  'size': 814041183},\n",
       " {'is_link_only': False,\n",
       "  'name': 'get_data.py',\n",
       "  'supplied_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "  'computed_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "  'id': 26766815,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26766815',\n",
       "  'size': 4113}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "data = json.loads(response.text)  # this contains all the articles data, feel free to check it out\n",
    "files = data[\"files\"]             # this is just the data about the files, which is what we want\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-voltage",
   "metadata": {},
   "source": [
    "The file we want is `data.zip`. We will download it with `urllib.request.urlretrieve()` then extract it with `zipfile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "combined-munich",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23min 47s\n"
     ]
    }
   ],
   "source": [
    "# I commented this out because I don't want to re-download data\n",
    "# Should be un-commented before submitting\n",
    "# \n",
    "# %%time\n",
    "# files_to_dl = [\"data.zip\"] \n",
    "# for file in files:\n",
    "#     if file[\"name\"] in files_to_dl:\n",
    "#         os.makedirs(output_directory, exist_ok=True)\n",
    "#         urlretrieve(file[\"download_url\"], output_directory + file[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "higher-jenny",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18.1 s\n"
     ]
    }
   ],
   "source": [
    "# I commented this out because I don't want to re-download data\n",
    "# Should be un-commented before submitting\n",
    "# \n",
    "# %%time\n",
    "# with zipfile.ZipFile(os.path.join(output_directory, \"data.zip\"), 'r') as f:\n",
    "#     f.extractall(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-proportion",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 4. Combining data CSVs\n",
    "There are now many `.csv` files that we want to merge into a single file. \n",
    "\n",
    "All the files but one have the same columns:\n",
    "- `time`\n",
    "- `lat_min`\n",
    "- `lat_max`\n",
    "- `lon_min`- \n",
    "- `lon_max`-\n",
    "- `rain (mm/day)`) \n",
    "\n",
    "The file `observed_daily_rainfall_SYD.csv` only has:\n",
    "- `time`\n",
    "- `rain (mm/day)`. \n",
    "\n",
    "I wish to keep all these columns and add data to the missing columns from `observed_daily_rainfall_SYD.csv` by looking up the latitude and longitude information for this data. It seems to be from Sydney. For now this missing data will appear as NaN's in the combined dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-messenger",
   "metadata": {},
   "source": [
    "### Using pandas to combine data\n",
    "One of the files, `observed_daily_rainfall_SYD.csv` was missing 4 columns the others had, so we made them and filled them with NaNs. Also we made a column 'model' that says which file the data came from. \n",
    "\n",
    "It took almost 7 minutes to combine and save the data as `combined_data.csv`, then about 1 minute to load the file back into a dataframe. This is a big dataframe, with over 62 million rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "developmental-terminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I commented this out because I don't want to re-download data\n",
    "# Should be un-commented before submitting\n",
    "# \n",
    "# use_cols = pd.read_csv(\"data/ACCESS-CM2_daily_rainfall_NSW.csv\", index_col=0).columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "signed-darwin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rain (mm/day)', 'lat_min', 'lat_max', 'lon_min', 'lon_max']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I commented this out because I don't want to re-download data\n",
    "# Should be un-commented before submitting\n",
    "# \n",
    "# obs_df = pd.read_csv(\"data/observed_daily_rainfall_SYD.csv\", index_col=0)\n",
    "# obs_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "southeast-monster",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rain (mm/day)', 'lat_min', 'lat_max', 'lon_min', 'lon_max']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I commented this out because I don't want to re-download data\n",
    "# Rerunning this cell would make duplicate rows and would mess things up\n",
    "# Should be un-commented before submitting\n",
    "# \n",
    "# # obs_df = pd.read_csv(\"data/observed_daily_rainfall_SYD.csv\")\n",
    "# obs_df.insert(1, \"lat_min\", np.nan, True)\n",
    "# obs_df.insert(2, \"lat_max\", np.nan, True)\n",
    "# obs_df.insert(3, \"lon_min\", np.nan, True)\n",
    "# obs_df.insert(4, \"lon_max\", np.nan, True)\n",
    "\n",
    "# obs_df.to_csv(\"data/observed_daily_rainfall_SYD.csv\")\n",
    "# obs_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "complimentary-consent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 292.93 MiB, increment: 0.07 MiB\n",
      "Wall time: 14min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit\n",
    "# combine data into a single giant `combined_data.csv` file\n",
    "\n",
    "files = glob.glob('data/*.csv')\n",
    "df = pd.concat((pd.read_csv(file, index_col=0)\n",
    "                .assign(model=re.findall(r'(?<=data\\\\)[^\\/]+(?=\\_d)', file)[0])\n",
    "                for file in files)\n",
    "              )\n",
    "df.to_csv(\"combined_data/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "sunrise-charleston",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 57.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv(\"combined_data/combined_data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "reserved-reminder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat_min</th>\n",
       "      <th>lat_max</th>\n",
       "      <th>lon_min</th>\n",
       "      <th>lon_max</th>\n",
       "      <th>rain (mm/day)</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1889-01-01 12:00:00</th>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>3.293256e-13</td>\n",
       "      <td>ACCESS-CM2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889-01-02 12:00:00</th>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>ACCESS-CM2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889-01-03 12:00:00</th>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>ACCESS-CM2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889-01-04 12:00:00</th>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>ACCESS-CM2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889-01-05 12:00:00</th>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>1.047658e-02</td>\n",
       "      <td>ACCESS-CM2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     lat_min  lat_max  lon_min  lon_max  rain (mm/day)  \\\n",
       "time                                                                     \n",
       "1889-01-01 12:00:00   -36.25    -35.0  140.625    142.5   3.293256e-13   \n",
       "1889-01-02 12:00:00   -36.25    -35.0  140.625    142.5   0.000000e+00   \n",
       "1889-01-03 12:00:00   -36.25    -35.0  140.625    142.5   0.000000e+00   \n",
       "1889-01-04 12:00:00   -36.25    -35.0  140.625    142.5   0.000000e+00   \n",
       "1889-01-05 12:00:00   -36.25    -35.0  140.625    142.5   1.047658e-02   \n",
       "\n",
       "                          model  \n",
       "time                             \n",
       "1889-01-01 12:00:00  ACCESS-CM2  \n",
       "1889-01-02 12:00:00  ACCESS-CM2  \n",
       "1889-01-03 12:00:00  ACCESS-CM2  \n",
       "1889-01-04 12:00:00  ACCESS-CM2  \n",
       "1889-01-05 12:00:00  ACCESS-CM2  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "consecutive-installation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62513863, 6)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-maximum",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "piano-victory",
   "metadata": {},
   "source": [
    "### Using DASK to combine data\n",
    "\n",
    "DASK took slightly longer  to combine the data into one .csv file compared to  pandas. \n",
    "\n",
    "DASK is able to load the dataframe much faster using `dd.read_csv()`.\n",
    "\n",
    "The `combined_data_dask.csv` file made from a DASK ddf takes up much more space on disk then the `combined_data.csv` file made from a pandas df. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fitted-scottish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 5815.75 MiB, increment: 5515.55 MiB\n",
      "Wall time: 10min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "ddf = dd.read_csv(\"data/*.csv\", \n",
    "#                   include_path_column = True,\n",
    "                  assume_missing = True)\n",
    "ddf.to_csv(\"combined_data/combined_data_dask.csv\", \n",
    "           single_file = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "actual-render",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 29.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ddf = dd.read_csv(\"combined_data/combined_data_dask.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sound-defensive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 790 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>time</th>\n",
       "      <th>lat_min</th>\n",
       "      <th>lat_max</th>\n",
       "      <th>lon_min</th>\n",
       "      <th>lon_max</th>\n",
       "      <th>rain (mm/day)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1889-01-01 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>3.293256e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1889-01-02 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1889-01-03 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1889-01-04 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1889-01-05 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>1.047658e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                 time  lat_min  lat_max  lon_min  lon_max  \\\n",
       "0           0  1889-01-01 12:00:00   -36.25    -35.0  140.625    142.5   \n",
       "1           1  1889-01-02 12:00:00   -36.25    -35.0  140.625    142.5   \n",
       "2           2  1889-01-03 12:00:00   -36.25    -35.0  140.625    142.5   \n",
       "3           3  1889-01-04 12:00:00   -36.25    -35.0  140.625    142.5   \n",
       "4           4  1889-01-05 12:00:00   -36.25    -35.0  140.625    142.5   \n",
       "\n",
       "   rain (mm/day)  \n",
       "0   3.293256e-13  \n",
       "1   0.000000e+00  \n",
       "2   0.000000e+00  \n",
       "3   0.000000e+00  \n",
       "4   1.047658e-02  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "metallic-river",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.4G\tcombined_data/combined_data_dask.csv\n",
      "5.7G\tcombined_data/combined_data.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "du -sh combined_data/combined_data_dask.csv\n",
    "du -sh combined_data/combined_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-chocolate",
   "metadata": {},
   "source": [
    "# 5. Load the combined CSV to memory and perform a simple EDA\n",
    "\n",
    "The EDA will be to count the number of data points that came from each .csv file, as recorded in the `model` column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-bikini",
   "metadata": {},
   "source": [
    "### 5.1 Pandas (no chunking)\n",
    "\n",
    "We will take this method to be a baseline with which to compare other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "robust-submission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 6731.93 MiB, increment: 6221.06 MiB\n",
      "Wall time: 58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "df = pd.read_csv(\"combined_data/combined_data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "piano-lawyer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI-ESM1-2-HR       5154240\n",
      "TaiESM1             3541230\n",
      "CMCC-ESM2           3541230\n",
      "CMCC-CM2-SR5        3541230\n",
      "CMCC-CM2-HR4        3541230\n",
      "NorESM2-MM          3541230\n",
      "SAM0-UNICON         3541153\n",
      "FGOALS-f3-L         3219300\n",
      "GFDL-ESM4           3219300\n",
      "GFDL-CM4            3219300\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "MRI-ESM2-0          3037320\n",
      "BCC-CSM2-MR         3035340\n",
      "MIROC6              2070900\n",
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "INM-CM4-8           1609650\n",
      "INM-CM5-0           1609650\n",
      "KIOST-ESM           1287720\n",
      "FGOALS-g3           1287720\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "AWI-ESM-1-1-LR       966420\n",
      "MPI-ESM1-2-LR        966420\n",
      "NESM3                966420\n",
      "NorESM2-LM           919800\n",
      "CanESM5              551880\n",
      "BCC-ESM1             551880\n",
      "observed              46020\n",
      "Name: model, dtype: int32\n",
      "peak memory: 3978.14 MiB, increment: 109.31 MiB\n",
      "Wall time: 4.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "counts = df[\"model\"].value_counts()\n",
    "print(counts.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-phase",
   "metadata": {},
   "source": [
    "### 5.2 Pandas (with chunking)\n",
    "\n",
    "This took slightly less time as pandas without chunking, but used much less memory. \n",
    "\n",
    "The effect of chunk size:\n",
    "- smaller chunk size (1,000,000) takes about the same time as larger (10,000,000)\n",
    "- smaller chunk size (1,000,000) uses less memory than larger (10,000,000)\n",
    "\n",
    "We could not time the loading of the data and performing the `value_counts()` (`.value_counts`) separately because the data from each chunk had to be counted as it became available. However the time  to do the combined operations using chunking was almost the same as doing the same operations in sequence without chunking.\n",
    "\n",
    "**Verdict:** Chunking decreases the memory needed but doesn't save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "tamil-terry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 3961.02 MiB, increment: 157.49 MiB\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "CHUNKSIZE = 1_000_000\n",
    "counts = pd.Series(dtype=int)\n",
    "for chunk in pd.read_csv(\"combined_data/combined_data.csv\", index_col=0,\n",
    "                         chunksize=CHUNKSIZE):\n",
    "    counts = counts.add(chunk[\"model\"].value_counts(), fill_value=0)\n",
    "# print(counts.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "practical-accounting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "AWI-ESM-1-1-LR       966420\n",
      "BCC-CSM2-MR         3035340\n",
      "BCC-ESM1             551880\n",
      "CMCC-CM2-HR4        3541230\n",
      "CMCC-CM2-SR5        3541230\n",
      "CMCC-ESM2           3541230\n",
      "CanESM5              551880\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "FGOALS-f3-L         3219300\n",
      "FGOALS-g3           1287720\n",
      "GFDL-CM4            3219300\n",
      "GFDL-ESM4           3219300\n",
      "INM-CM4-8           1609650\n",
      "INM-CM5-0           1609650\n",
      "KIOST-ESM           1287720\n",
      "MIROC6              2070900\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "MPI-ESM1-2-HR       5154240\n",
      "MPI-ESM1-2-LR        966420\n",
      "MRI-ESM2-0          3037320\n",
      "NESM3                966420\n",
      "NorESM2-LM           919800\n",
      "NorESM2-MM          3541230\n",
      "SAM0-UNICON         3541153\n",
      "TaiESM1             3541230\n",
      "observed              46020\n",
      "dtype: int32\n",
      "peak memory: 5947.30 MiB, increment: 2110.93 MiB\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "CHUNKSIZE = 10_000_000\n",
    "counts = pd.Series(dtype=int)\n",
    "for chunk in pd.read_csv(\"combined_data/combined_data.csv\", chunksize=CHUNKSIZE):\n",
    "    counts = counts.add(chunk[\"model\"].value_counts(), fill_value=0)\n",
    "print(counts.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-original",
   "metadata": {},
   "source": [
    "### 5.3 Pandas (loading only some columns, no chunking)\n",
    "Since we only want the model for EDA, we will import just the `model` column. This is faster and uses less memory than loading the whole dataframe. \n",
    "\n",
    "Running `value_counts` takes the same time as it did using the entire data set. Probably because it has to iterate through the same number of rows.\n",
    "\n",
    "**Verdict:** This should be done whenever possible. It reduces memory and speeds up loading data but has no effect on EDA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sustainable-mambo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 4860.36 MiB, increment: 957.31 MiB\n",
      "Wall time: 33.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "df = pd.read_csv(\"combined_data/combined_data.csv\", \n",
    "                 usecols = [\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "smoking-expansion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 1099.74 MiB, increment: 58.92 MiB\n",
      "Wall time: 4.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "df[\"model\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-murder",
   "metadata": {},
   "source": [
    "### 5.4 DASK dataframe\n",
    "\n",
    "This method is much faster than using pandas (without chunking), and uses less memory. The memory use is similar to using pandas with chunking, depending on the chunk size used. The `value_counts()` step took longer than the baseline pandas method because of the `.compute()` step, which is required when we want to work with the data and view the result.\n",
    "\n",
    "**Verdict:** Fast and light on memory. Similar to chunked pandas but easier to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "northern-magazine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 7021.62 MiB, increment: 7.30 MiB\n",
      "Wall time: 2.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# load data\n",
    "ddf = dd.read_csv(\"combined_data/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "spectacular-holmes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI-ESM1-2-HR       5154240\n",
      "TaiESM1             3541230\n",
      "CMCC-CM2-HR4        3541230\n",
      "CMCC-CM2-SR5        3541230\n",
      "CMCC-ESM2           3541230\n",
      "NorESM2-MM          3541230\n",
      "SAM0-UNICON         3541153\n",
      "FGOALS-f3-L         3219300\n",
      "GFDL-CM4            3219300\n",
      "GFDL-ESM4           3219300\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "MRI-ESM2-0          3037320\n",
      "BCC-CSM2-MR         3035340\n",
      "MIROC6              2070900\n",
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "INM-CM4-8           1609650\n",
      "INM-CM5-0           1609650\n",
      "KIOST-ESM           1287720\n",
      "FGOALS-g3           1287720\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "MPI-ESM1-2-LR        966420\n",
      "NESM3                966420\n",
      "AWI-ESM-1-1-LR       966420\n",
      "NorESM2-LM           919800\n",
      "CanESM5              551880\n",
      "BCC-ESM1             551880\n",
      "observed              46020\n",
      "Name: model, dtype: int64\n",
      "peak memory: 8592.94 MiB, increment: 1575.92 MiB\n",
      "Wall time: 38.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# Do EDA\n",
    "print(ddf[\"model\"].value_counts().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normal-reader",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-possession",
   "metadata": {},
   "source": [
    "### 5.5 Changing dtype of the data\n",
    "\n",
    "Four of the columns are in float64 format so they could be converted to float32 and reduce the memory required by 1.25GB. \n",
    "\n",
    "**Verdict:** Should be done regardless of the method used to store and import data. This is low-hanging fruit to save space and should be used whenever possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fitted-bandwidth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage with float64: 2500.55 MB\n",
      "Memory usage with float32: 1250.28 MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"combined_data/combined_data.csv\")\n",
    "\n",
    "print(f\"Memory usage with float64: {df[['lat_min', 'lat_max', 'lon_min', 'lon_max', 'rain (mm/day)']].memory_usage().sum() / 1e6:.2f} MB\")\n",
    "print(f\"Memory usage with float32: {df[['lat_min', 'lat_max', 'lon_min', 'lon_max', 'rain (mm/day)']].astype('float32', errors='ignore').memory_usage().sum() / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-stone",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "found-kentucky",
   "metadata": {},
   "source": [
    "# 6. Perform a simple EDA in R\n",
    "To do this I will pass data from python to R in various ways then decide which one is best suited to this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "colored-nitrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(dplyr)\n",
    "library(arrow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-celebration",
   "metadata": {},
   "source": [
    "### 6.1 Using %%R -i\n",
    "\n",
    "This didn't work because it seems my laptop didn't have enough memory. This doesn't seem to be a good way to pass a large dataframe from python to R.\n",
    "\n",
    "**Verdict: NO THANKS!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "inside-river",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# %%R -i df\n",
    "\n",
    "# start_time <- Sys.time()\n",
    "# library(dplyr)\n",
    "# counts <- df %>% count(model)\n",
    "# end_time <- Sys.time()\n",
    "\n",
    "# print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-receiver",
   "metadata": {},
   "source": [
    "### 6.2 Using arrow table and pyra\n",
    "\n",
    "The arrow table is an intermediary form that can easily be moved between python and R. Once it is made it can be convert it using `pyra.converter.py2rpy()` to a table that is readble in R. This is a rather roundabout way to get data into R. It would be nicer to be able to load a file directly into R.\n",
    "\n",
    "To create a table from file in python, convert the table, then pass to R took about the same time as loading the file and doing the EDA in python. This is impressive, since it took the same amount of time to do much more. \n",
    "\n",
    "**Verdict: Ok, but inconvenient.** This method works and but it is inconvenient to have to create a table in python, convert it, then pass that to R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "varied-first",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 4164.01 MiB, increment: 3872.60 MiB\n",
      "Wall time: 28.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# Read data file and prepare arrow table\n",
    "dataset = ds.dataset(\"combined_data/combined_data.csv\", format=\"csv\")\n",
    "table = dataset.to_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "constant-power",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5756\n",
      "rarrow.ChunkedArray: 0.037856340408325195\n",
      "5756\n",
      "rarrow.ChunkedArray: 0.03503870964050293\n",
      "5756\n",
      "rarrow.ChunkedArray: 0.02499079704284668\n",
      "5756\n",
      "rarrow.ChunkedArray: 0.011687278747558594\n",
      "5756\n",
      "rarrow.ChunkedArray: 0.04680037498474121\n",
      "5756\n",
      "rarrow.ChunkedArray: 0.03125166893005371\n",
      "5756\n",
      "rarrow.ChunkedArray: 0.03126835823059082\n",
      "peak memory: 4242.32 MiB, increment: 78.30 MiB\n",
      "Wall time: 23.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# convert arrow table so it can be passed to R\n",
    "r_table = pyra.converter.py2rpy(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "attached-encyclopedia",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: \n",
      "Attaching package: 'dplyr'\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from 'package:stats':\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from 'package:base':\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# A tibble: 28 x 2\n",
      "   model                  n\n",
      "   <chr>              <int>\n",
      " 1 ACCESS-CM2       1932840\n",
      " 2 ACCESS-ESM1-5    1610700\n",
      " 3 AWI-ESM-1-1-LR    966420\n",
      " 4 BCC-CSM2-MR      3035340\n",
      " 5 BCC-ESM1          551880\n",
      " 6 CanESM5           551880\n",
      " 7 CMCC-CM2-HR4     3541230\n",
      " 8 CMCC-CM2-SR5     3541230\n",
      " 9 CMCC-ESM2        3541230\n",
      "10 EC-Earth3-Veg-LR 3037320\n",
      "# ... with 18 more rows\n",
      "Time difference of 4.852329 secs\n",
      "Wall time: 5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R -i r_table\n",
    "# Pass r_table from python\n",
    "\n",
    "start_time <- Sys.time()\n",
    "library(dplyr)\n",
    "counts <- r_table %>% collect() %>% count(model)\n",
    "end_time <- Sys.time()\n",
    "\n",
    "print(counts)\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-acting",
   "metadata": {},
   "source": [
    "### 6.3 Using feather\n",
    "If we have an arrow table it can be saved as a .feather file, which R can read directly. This is convenient... **not working... need to fix and add comments**\n",
    "\n",
    "**Verdict:** ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "needed-florist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create .feather file from existing arrow table\n",
    "feather.write_feather(table, 'combined_data/combined_data.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-actress",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "# Read .feather file into R\n",
    "library(arrow)\n",
    "start_time <- Sys.time()\n",
    "r_table <- arrow::read_feather(\"combined_data/combined_data.feather\")\n",
    "end_time <- Sys.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-default",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "# Perform EDA in R\n",
    "library(dplyr)\n",
    "start_time <- Sys.time()\n",
    "counts <- r_table %>% collect() %>% count(model)\n",
    "end_time <- Sys.time()\n",
    "\n",
    "print(counts)\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-loading",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "primary-terror",
   "metadata": {},
   "source": [
    "### 6.4 Using parquet\n",
    "\n",
    "This format results in small file size, but reading and writing data is slower than arrow formats, according to the Apache Arrow people: https://ursalabs.org/blog/2020-feather-v2/\n",
    "\n",
    "I don't know how to read this into R and it's late and I'm tired so I'm going to let this slide for now.\n",
    "\n",
    "**Verdict:** File size is very small, but we didn't test the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-novelty",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pq.write_table(table, 'combined_data/combined_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-chile",
   "metadata": {},
   "source": [
    "#### Compare filesize of various formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-prize",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "du -sh combined_data/combined_data.csv\n",
    "du -sh combined_data/combined_data.feather\n",
    "du -sh combined_data/combined_data.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-point",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:525]",
   "language": "python",
   "name": "conda-env-525-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
